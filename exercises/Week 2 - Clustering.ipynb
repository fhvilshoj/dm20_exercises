{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 (K-means, K-medoids, Gaussian Mixtures)\n",
    "\n",
    "This week we are going to work with K-means, K-medoids, and Gaussian Mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Local imports (used for the last optional exercise.)\n",
    "import math\n",
    "import itertools\n",
    "import sys\n",
    "sys.path.append(\"../utilities\")\n",
    "from gmm import GMM\n",
    "from load_data import load_iris, load_iris_PC, index_to_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Warmup\n",
    "Please provide a brief description of what characterises \n",
    "1. Clustering as a task \n",
    "2. Representative-based clustering as a clustering approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Practical K-means\n",
    "Given the following points: 2, 4, 10, 12, 3, 20, 30, 11, 25. Assume $k=3$, and that we randomly pick the initial means $\\mu_1=2$, $ \\mu_2=4$ and $\\mu_3=6$. Show the clusters obtained using K-means algorithm after one iteration, and show the new means for the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use python if you want but it is not required!\n",
    "X = np.array([2, 4, 10, 12, 3, 20, 30, 11, 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Which algorithm is more robust: k-means or k-medoid and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Practical Mixture of Gaussians\n",
    "Given the data points in table below, and their probability of belonging to two clusters.\n",
    "Assume that these points were produced by a mixture of two univariate normal distributions. \n",
    "Answer the following questions:\n",
    "\n",
    "1. Find the maximum likelihood estimate of the means $\\mu_1$ and $\\mu_2$\n",
    "2. Assume that $\\mu_1 = 2$, $\\mu_2 = 7$, and $\\sigma_1 = \\sigma_2 = 1$. Fint the probability that the point $x=5$ belongs to cluster $C_1$ and to cluster $C_2$. You may assume that the prior probability of each cluster is equal (i.e., $P(C_1) = P(C_2) = 0.5$), and the prior probability $P(x=5) = 0.029$\n",
    "\n",
    "|$x$|$P(C_1|x)$|$P(C_2|x)$|\n",
    "|:---:|:---:|:---:|\n",
    "| --- | ---------------- | ---------------- |\n",
    "|2 |  0.9  |  0.1  |\n",
    "|3|0.8|0.2|\n",
    "|7|0.3|0.7|\n",
    "|9|0.1|0.9|\n",
    "|2|0.9|0.1|\n",
    "|1|0.8|0.2|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, you can use python here.\n",
    "# Note that there is a\n",
    "X            = np.array([2, 3, 7, 9, 2, 1])\n",
    "P_C1_given_x = np.array([0.9, 0.8, 0.3, 0.1, 0.9, 0.8])\n",
    "P_C2_given_x = 1 - P_C1_given_x\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "For which parameter settings is EM clustering identical to k-means clustering and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: 2d K-means and gaussian mixture\n",
    "Given the two-dimensional points in Table 13.2, assume that $k=2$, and that initially the points are assigned to clusters as follors: $C_1 = \\{ x_1, x_2, x_4 \\}$ and $C_2 = \\{ x_3, x_5 \\}$.\n",
    "Answer the following questions:\n",
    "\n",
    "1. Apply the K-means algorithm until convergence, that is, the clusters do not change, assuming (1) the usual Euclidean distance of the $L_2$-norm as the distance between points, defined as\n",
    "\n",
    "$$\n",
    "||x_i - x_j||_2 = \\sqrt{ \\sum_{a=1}^d (x_{ia} - x_{ja})^2 }\n",
    "$$\n",
    " and (2) the Manhattan distance of the $L_1$-norm\n",
    "$$\n",
    "||x_i - x_j||_1 = \\sum_{a=1}^d |x_{ia} - x_{ja}|.\n",
    "$$\n",
    "\n",
    "2. Apply the EM algorithm with $k=2$ assuming that the dimensions are independent. Show one complete execution of the expectation and the maximization steps. Start with the assumption that $P(C_i | x_{ja}) = 0.5$ for $a=1, 2$ and $j=1, ..., 5$.\n",
    "\n",
    "\n",
    "![Table 13.2](graphics/13.2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, if you want, you can use a bit of Python\n",
    "X = np.array([\n",
    "    [0, 0, 1.5, 5, 5],\n",
    "    [2, 0,   0, 0, 2]\n",
    "]).T # shape [5, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionals\n",
    "## Exercise 7\n",
    "Consider 2D data (2,2), (2,1), (2,3), (1,2), (3,2), (8,2), (8,1), (8,0), (8,3), (8,4), (7,2), (6,2), (9,2), (10,2), (7,1), (7,3), (9,1), (9,3)  \n",
    "\n",
    "![Data plotted](graphics/two_cluster_dataplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let k=2 and sketch visually what you think the final clustering will be and explain why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Gaussian Mixtures and the EM-Algorithm\n",
    "In this exercise, we will implement the EM-algorithm for the Gaussian Mixture Model.\n",
    "You can consult [Zaki] Section 13.3.2, for a description of how the algorithm works in this particular setup.\n",
    "\n",
    "Below is an extension of a Gaussian Mixture Model stub (`GMM`) found [here](../utilities/gmm.py), which has the method signatures for the unimplemented functions. Try to fill out the methods and run the experiment below afterwards.\n",
    "\n",
    "Besides the methods shown here, the `GMM` class has both a `fit` and a `predict` method, which both takes as input the data and returns `void` and cluster indexes, respectively. Both will use the functions that you implement below. Additionally, the number of gaussian mixtures `K` can be accessesed by `self.K`.\n",
    "\n",
    "Finally, the `GMM` class has a static function `prob`, which returns the values of a Gaussian pdf, given data, mean, and covariance matrix; use it if you please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGMM(GMM):\n",
    "    def initialize_parameters(self, X):\n",
    "        \"\"\"\n",
    "            This function should utilize information from the data to initialize\n",
    "            the parameters of the model.\n",
    "            In particular, it should compute initial values for mu, Sigma, and pi.\n",
    "            \n",
    "            The function corresponds to line 2-4 in Algorithm 13.3 in [Zaki, p. 349]\n",
    "            Note, that K can be retrieved as `self.K`.\n",
    "\n",
    "            Args:\n",
    "                X (matrix, [n, d]): Data to be used for initialization.\n",
    "\n",
    "            Returns:\n",
    "                Tuple (mu, Sigma, pi), \n",
    "                    mu has size        [K, d]\n",
    "                    Sigma has size     [K, d, d]\n",
    "                    pi has size        [K]\n",
    "        \"\"\"\n",
    "        # TODO: what should the values be for initializing mu, Sigma and pi\n",
    "        return mu, Sigma, pi\n",
    "\n",
    "\n",
    "    def posterior(self, X):\n",
    "        \"\"\"\n",
    "            The E-step of the EM algorithm. \n",
    "            Returns the posterior probability p(Y|X)\n",
    "\n",
    "            This function corresponds to line 8 in Algorithm 13.3 in [Zaki, p. 349]\n",
    "            Note, that mean and covariance matrices can be accessed by `self.mu` and `self.Sigma`, respectively.\n",
    "            \n",
    "            Args:\n",
    "                X (matrix, [n,  d]): Data to compute posterior for.\n",
    "\n",
    "            Returns:\n",
    "                Matrix of size        [n, K]\n",
    "        \"\"\"\n",
    "        # TODO: what is the posterior probability?\n",
    "        \n",
    "        return posterior\n",
    "        \n",
    "\n",
    "    def m_step(self, X, P):\n",
    "        \"\"\"\n",
    "            Update the estimates of mu, Sigma, and pi, given the data `X` and the current\n",
    "            posterior probabilities `P`.\n",
    "\n",
    "            This function corresponds to line 10-12 in Algorithm 13.3 and Eqn. (13.11-13) in [Zaki, p. 349].\n",
    "            \n",
    "            Args:\n",
    "                X (matrix, [n, d]): Data matrix\n",
    "                P (matrix, [n, K]): The posterior probabilities for the n samples.\n",
    "\n",
    "            Returns:\n",
    "                Tuple (mu, Sigma, pi), \n",
    "                    mu has size        [K, d]\n",
    "                    Sigma has size    [K, d, d]\n",
    "                    pi has size        [K]\n",
    "        \"\"\"\n",
    "        # TODO: what is the values of mu, Sigma, and pi that maximizes the expectation given the posterior?\n",
    "        return  mu_hat, Si_hat, pi_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the data used for the experiments to see the actual classes \n",
    "def plot_iris(X, y, title=''):\n",
    "    # Plotting\n",
    "    _, d = X.shape\n",
    "    \n",
    "    combinations = list(itertools.combinations(np.arange(d), 2))\n",
    "    \n",
    "    cols    = min(3, len(combinations) )\n",
    "    rows    = math.ceil(len(combinations)/cols)\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    \n",
    "    if len(title) > 0: fig.suptitle(title)\n",
    "    \n",
    "    # Fix indexing when there are few plots.\n",
    "    if rows == 1: ax = [ax]\n",
    "    if cols == 1: ax = [ax]\n",
    "\n",
    "    c       = 0\n",
    "    for i, j in combinations:\n",
    "        m = c // cols\n",
    "        n = c % cols\n",
    "        ax[m][n].scatter(X[:,i], X[:,j], c=y)\n",
    "        ax[m][n].set_xlabel(index_to_feature[i])\n",
    "        ax[m][n].set_ylabel(index_to_feature[j])\n",
    "        c += 1 \n",
    "    # fig.tight_layout()\n",
    "\n",
    "# Load the Iris data set\n",
    "X , y    = load_iris()\n",
    "X_, y_   = load_iris_PC()\n",
    "\n",
    "plot_iris(X, y, 'Iris')\n",
    "plot_iris(X_, y_, 'Iris 2 Principal Components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs your implementation of the GMM on the simple 2D Iris data and plots it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny experiment 2PC\n",
    "K       = 3\n",
    "gmm     = MyGMM(K)\n",
    "gmm.fit(X_, max_iter=100)\n",
    "\n",
    "plot_iris(X_, y_, 'Real labels')\n",
    "plot_iris(X_, gmm.predict(X_), \"Labels from GMM model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs your implementation of the GMM on the full 4D Iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All four dimensions iris\n",
    "K       = 3\n",
    "gmm     = MyGMM(K)\n",
    "gmm.fit(X, max_iter=100)\n",
    "plot_iris(X, y, 'Real labels')\n",
    "plot_iris(X, gmm.predict(X), \"Labels from GMM model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
